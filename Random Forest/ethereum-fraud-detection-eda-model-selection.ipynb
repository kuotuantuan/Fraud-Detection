{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## This dataset contains rows of known fraud and valid transactions made over Ethereum.\n\n### Task: EDA & Prediction of Fraud/Valid Transaction\n\nHere is a description of the rows of the dataset:\n* Index: the index number of a row\n* Address: the address of the ethereum account\n* FLAG: whether the transaction is fraud or not\n* Avg min between sent tnx: Average time between sent transactions for account in minutes\n* Avgminbetweenreceivedtnx: Average time between received transactions for account in minutes\n* TimeDiffbetweenfirstand_last(Mins): Time difference between the first and last transaction\n* Sent_tnx: Total number of sent normal transactions\n* Received_tnx: Total number of received normal transactions\n* NumberofCreated_Contracts: Total Number of created contract transactions\n* UniqueReceivedFrom_Addresses: Total Unique addresses from which account received transactions\n* UniqueSentTo_Addresses20: Total Unique addresses from which account sent transactions\n* MinValueReceived: Minimum value in Ether ever received\n* MaxValueReceived: Maximum value in Ether ever received\n* AvgValueReceived5Average value in Ether ever received\n* MinValSent: Minimum value of Ether ever sent\n* MaxValSent: Maximum value of Ether ever sent\n* AvgValSent: Average value of Ether ever sent\n* MinValueSentToContract: Minimum value of Ether sent to a contract\n* MaxValueSentToContract: Maximum value of Ether sent to a contract\n* AvgValueSentToContract: Average value of Ether sent to contracts\n* TotalTransactions(IncludingTnxtoCreate_Contract): Total number of transactions\n* TotalEtherSent:Total Ether sent for account address\n* TotalEtherReceived: Total Ether received for account address\n* TotalEtherSent_Contracts: Total Ether sent to Contract addresses\n* TotalEtherBalance: Total Ether Balance following enacted transactions\n* TotalERC20Tnxs: Total number of ERC20 token transfer transactions\n* ERC20TotalEther_Received: Total ERC20 token received transactions in Ether\n* ERC20TotalEther_Sent: Total ERC20token sent transactions in Ether\n* ERC20TotalEtherSentContract: Total ERC20 token transfer to other contracts in Ether\n* ERC20UniqSent_Addr: Number of ERC20 token transactions sent to Unique account addresses\n* ERC20UniqRec_Addr: Number of ERC20 token transactions received from Unique addresses\n* ERC20UniqRecContractAddr: Number of ERC20token transactions received from Unique contract addresses\n* ERC20AvgTimeBetweenSent_Tnx: Average time between ERC20 token sent transactions in minutes\n* ERC20AvgTimeBetweenRec_Tnx: Average time between ERC20 token received transactions in minutes\n* ERC20AvgTimeBetweenContract_Tnx: Average time ERC20 token between sent token transactions\n* ERC20MinVal_Rec: Minimum value in Ether received from ERC20 token transactions for account\n* ERC20MaxVal_Rec: Maximum value in Ether received from ERC20 token transactions for account\n* ERC20AvgVal_Rec: Average value in Ether received from ERC20 token transactions for account\n* ERC20MinVal_Sent: Minimum value in Ether sent from ERC20 token transactions for account\n* ERC20MaxVal_Sent: Maximum value in Ether sent from ERC20 token transactions for account\n* ERC20AvgVal_Sent: Average value in Ether sent from ERC20 token transactions for account\n* ERC20UniqSentTokenName: Number of Unique ERC20 tokens transferred\n* ERC20UniqRecTokenName: Number of Unique ERC20 tokens received\n* ERC20MostSentTokenType: Most sent token for account via ERC20 transaction\n* ERC20MostRecTokenType: Most received token for account via ERC20 transactions","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport klib\n\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, learning_curve, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import f1_score, roc_auc_score, plot_confusion_matrix, accuracy_score\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-13T22:43:13.925129Z","iopub.execute_input":"2022-10-13T22:43:13.925858Z","iopub.status.idle":"2022-10-13T22:43:14.376222Z","shell.execute_reply.started":"2022-10-13T22:43:13.925817Z","shell.execute_reply":"2022-10-13T22:43:14.374598Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f08102941e01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mklib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'klib'"],"ename":"ModuleNotFoundError","evalue":"No module named 'klib'","output_type":"error"}]},{"cell_type":"code","source":"df = pd.read_csv('../input/ethereum-frauddetection-dataset/transaction_dataset.csv', index_col=[0])\ndf.drop(columns='Index', inplace=True)\ndf.drop_duplicates(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sampling some observations\nprint(df.shape)\n# df.sample(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# column names\n# display(df.columns)\n\n# describtion of numeratic columns\n# display(df.describe())\n\n# Non-Null Count and type of columns\ndisplay(df.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical columns","metadata":{}},{"cell_type":"code","source":"zero_feature_list = df.columns[(df.nunique() == 1)].tolist()\nzero_feature_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_list = list(set(df.columns) - set(['Address', 'FLAG']) - set(zero_feature_list))\nnum_feature_list = list(set(feature_list) - set(df.dtypes[df.dtypes == 'object'].index) - set(zero_feature_list))\ncat_feature_list = list(set(feature_list) - set(num_feature_list))\n\nassert len(feature_list) == len(num_feature_list) + len(cat_feature_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the correlation with all other numeric variables in the raw data set\nklib.corr_plot(df, target='FLAG')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# non-unique adresses\n_, uniq_idx, counts = np.unique(ds['Address'].to_numpy(), return_index=True, return_counts=True)\nnon_unique_addresses_idx = uniq_idx[counts > 1]\nprint(\"non-unique adresses count: {}\".format(len(non_unique_addresses_idx), end='\\n\\n'))\n# What are the flags of non-uniqe adresses\nnon_unique_addresses_flags = ds.iloc[non_unique_addresses_idx]['FLAG']\nprint(\"flags of non-unique adresses: \", end='')\nprint(*non_unique_addresses_flags)\nds.head(1)\n# ds.drop(columns='Address', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Columns 'ERC20 most sent token type' and ' ERC20_most_rec_token_type' contain token types. Most of tokens occur only once so they seem irrelevant in fraud detection.","metadata":{}},{"cell_type":"code","source":"display(np.unique(ds[' ERC20 most sent token type'].astype(str)))\ndisplay(np.unique(ds[' ERC20_most_rec_token_type'].astype(str)))\n\n# ds.drop(columns=[' ERC20 most sent token type', ' ERC20_most_rec_token_type'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numerical columns","metadata":{}},{"cell_type":"code","source":"# are classes balanced?\nprint('class : count : percent')\nprint('0     : {}  : {:.2%}'.format(sum(ds['FLAG']==0), sum(ds['FLAG']==0)/len(ds['FLAG']) ))\nprint('1     : {}  : {:.2%}'.format(sum(ds['FLAG']==1), sum(ds['FLAG']==1)/len(ds['FLAG']) ))\n\nsns.heatmap(ds.iloc[:,0])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset is unbalanced, we must remember about this when choosing our model metric.","metadata":{}},{"cell_type":"markdown","source":"Deleting columns that holds only zeros.","metadata":{}},{"cell_type":"code","source":"ds.drop(columns=[' ERC20 avg time between rec tnx', ' ERC20 avg time between rec 2 tnx', ' ERC20 avg time between contract tnx',\n                 ' ERC20 min val sent contract', ' ERC20 max val sent contract', ' ERC20 avg val sent contract', ' ERC20 avg time between sent tnx'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing values\nmissing_values = ds.isna()\nmissing_percent = missing_values.sum() / ds.shape[0] * 100\nmissing_df = pd.DataFrame([missing_values.sum(), missing_percent], ['count', 'percent'])\ndisplay(missing_df.sort_values(by='percent', axis=1, ascending=False))\nmissing_df.sort_values(by='percent', axis=1, ascending=False).to_csv('missing.csv')\n\nsns.heatmap(missing_values, cbar=False, cmap='magma')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like missing values are highly connected to fraud cases","metadata":{}},{"cell_type":"code","source":"non_fraud_rows, fraud_rows = np.where( [ds.iloc[:,0]==1] )\nprint(ds.iloc[fraud_rows,:].isna().sum()[-20:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we expected every missing value is in fraud rows. That means that almost 40% of fraud rows have missing values.","metadata":{}},{"cell_type":"code","source":"missing_columns = ds.columns[ds.isna().sum() > 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation\ncorr = ds.corr()\nplt.figure(figsize=(20,12))\nsns.heatmap(np.abs(corr), cmap='coolwarm')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{}},{"cell_type":"markdown","source":"We left only numeratic features so preprocessing is limited to imputing null values with column mean and scaling.","metadata":{}},{"cell_type":"code","source":"preprocessing_pipeline = Pipeline([\n    ('impoter', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\nX = ds.drop(columns='FLAG').to_numpy()\ny = ds['FLAG'].to_numpy()\n\nrandom_permutation = np.random.permutation(len(X))\nX = X[random_permutation]\ny = y[random_permutation]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n\nX_train = preprocessing_pipeline.fit_transform(X_train)\nX_test = preprocessing_pipeline.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model selection","metadata":{}},{"cell_type":"code","source":"def evaluate_models(X, y, models, cv):\n    f1_scores = dict()\n    acc_scores = dict()\n    \n    for i, model in enumerate(models):\n        clf_pipeline = make_pipeline(preprocessing_pipeline, model)\n        results = cross_validate(clf_pipeline, X, y, cv=cv, scoring=['f1', 'accuracy'], n_jobs=-1)\n        avg_f1 = np.mean(results['test_f1'])\n        avg_acc = np.mean(results['test_accuracy'])\n        \n        model_name = model.__class__.__name__\n        f1_scores[model_name] = avg_f1\n        acc_scores[model_name] = avg_acc\n        print('{}-of-{}: {} f1={}, acc={}'.format(i+1, len(models), model_name, avg_f1, avg_acc))\n    return f1_scores, acc_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = StratifiedKFold(5, shuffle=True, random_state=42)\n\nclassifiers = [\n    LogisticRegression(random_state=42),\n    KNeighborsClassifier(),\n    RandomForestClassifier(random_state=42),\n    lgb.LGBMClassifier(random_state=42),\n    xgb.XGBClassifier(random_state=42),\n    SVC(random_state=42),\n    AdaBoostClassifier(random_state=42),\n    GaussianNB(),\n    MLPClassifier(random_state=42),\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_scores, acc_scores = evaluate_models(X, y, classifiers, cv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_scores(f1_scores, acc_scores):\n    x = np.arange(len(f1_scores))\n    width = 0.45\n    \n    f1_values = list(f1_scores.values())\n    acc_values = list(acc_scores.values())\n    \n    plt.figure(figsize=(15, 8)).tight_layout()\n    plt.bar(x - width / 2, f1_values, width, label='f1')\n    plt.bar(x + width / 2, acc_values, width, label='accuracy')\n    \n    for index, value in enumerate(x - width / 2):\n        plt.text(value, f1_values[index], '{:.3}'.format(f1_values[index]),\n                 verticalalignment='bottom', horizontalalignment='center', fontsize=10)\n\n    for index, value in enumerate(x + width / 2):\n        plt.text(value, acc_values[index], '{:.3}'.format(acc_values[index]),\n                 verticalalignment='bottom', horizontalalignment='center', fontsize=10)    \n    \n    classifiers_names = f1_scores.keys()\n    plt.xticks(x, classifiers_names, rotation=40, horizontalalignment='right', fontsize=10)\n    plt.legend()\n\nvisualize_scores(f1_scores, acc_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBClassifier hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"xgb_parameters = {\n    'xgbclassifier__n_estimators': range(1000, 4001, 1000),\n    'xgbclassifier__gamma': [0, 0.5, 1],\n    'xgbclassifier__max_depth': [5, 6, 7]\n}\n\nxgb_pipeline = make_pipeline(preprocessing_pipeline, xgb.XGBClassifier(random_state=42))\n# xgb_pipeline.steps\nxgb_grid_search = RandomizedSearchCV(\n    xgb_pipeline,\n    param_distributions=xgb_parameters,\n    scoring = 'f1',\n    n_iter = 12,\n    n_jobs = -1,\n    cv = 5,\n    random_state=42\n)\n\nxgb_grid_search.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#grid_search.grid_scores_\ndisplay(xgb_grid_search.best_score_)\ndisplay(xgb_grid_search.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LGBM_parameters = {\n        'lgbmclassifier__bagging_fraction': [0, 0.2, 0.5, 0.8, 1],\n        'lgbmclassifier__feature_fraction': [0.5, 0.8],\n        'lgbmclassifier__max_depth': [6, 10, 13, 16, 20],\n        'lgbmclassifier__min_data_in_leaf': range(40, 180, 20),\n        'lgbmclassifier__num_leaves': range(500, 2500, 300)\n}\n\nLGBM_pipeline = make_pipeline(preprocessing_pipeline, lgb.LGBMClassifier(random_state=42))\nLGBM_grid_search = RandomizedSearchCV(\n    LGBM_pipeline,\n    param_distributions=LGBM_parameters,\n    scoring = 'f1',\n    n_iter = 60,\n    n_jobs = -1,\n    cv = 5,\n    random_state=42\n)\n\nLGBM_grid_search.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(LGBM_grid_search.best_score_)\ndisplay(LGBM_grid_search.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RFC_parameters = {\n        'randomforestclassifier__n_estimators': range(50, 1050, 100),\n        'randomforestclassifier__max_depth': range(50, 300, 20),\n        'randomforestclassifier__min_samples_split': [2, 5, 10],\n        'randomforestclassifier__min_samples_leaf': [1, 2, 4],\n        'randomforestclassifier__bootstrap': [True, False]\n}\n\nRFC_pipeline = make_pipeline(preprocessing_pipeline, RandomForestClassifier(random_state=42))\nRFC_grid_search = RandomizedSearchCV(\n    RFC_pipeline,\n    param_distributions=RFC_parameters,\n    scoring = 'f1',\n    n_iter = 24,\n    n_jobs = -1,\n    cv = 5,\n    random_state=42\n)\n\nRFC_grid_search.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(RFC_grid_search.best_score_)\ndisplay(RFC_grid_search.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Best model evaluation","metadata":{}},{"cell_type":"code","source":"best_model = lgb.LGBMClassifier(num_leaves=1400, min_data_in_leaf=100, max_depth=10,\n    feature_fraction=0.8, bagging_fraction=0.5, random_state=42)\n\nbest_model.fit(X_train, y_train)\n\npredictions = best_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"f1 score = {}\".format(f1_score(y_test, predictions)))\n\nprint(\"ROC AUC score = {}\".format(roc_auc_score(y_test, predictions)))\n\nprint(\"accuracy score = {}\".format(accuracy_score(y_test, predictions)))\n\ndisplay(plot_confusion_matrix(best_model, X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_features_importance(feature_importance):\n    column_names = ds.drop(columns='FLAG').columns\n\n    df_feature_importance = pd.DataFrame(sorted(zip(feature_importance, column_names)),\n                                       columns=['Importance value', 'Feature'])\n    df_feature_importance = df_feature_importance.sort_values('Importance value', ascending=False)\n\n    plt.figure(figsize=(9, 7)).tight_layout()\n    sns.barplot(y=\"Feature\", x=\"Importance value\", data=df_feature_importance)\n    plt.show()\n\nplot_features_importance(best_model.feature_importances_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}